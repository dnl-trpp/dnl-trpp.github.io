---
layout: post
title:  "Exhausting the keyspace with multiple valid tokens."
description: "How generating multiple login token allows to takeover user accounts"
date:   2025-09-06 16:00:00 +0100
categories: writeup
image:
  path: /assets/img/dnl-airport.jpg
  alt: Me at the airport, so focused on my screen that I don't notice my flight is half an hour late
---

# An (unecessary) deep dive on exhausting the keyspace with multiple tokens.

While this section is completely not needed for the understanding of the vulnerability, it's the reasonings contained here that led me to write this article in the first place. 

The reason is that while testing, I asked myself the following question:
> Given a fixed number of requests I can make over the network, is it better to generate a new valid token or taking an attempt at guessing one?

In other words, is generating a new valid OTP token (without invalidating the previous one) a more powerful primitive than simply having the possibility to take a guess at the OTP token? 

The short answer is yes, an maybe it's not surprising even obvious to some, but it was not to me immediately. In fact let's suppose we can only make a total of 10 network requests and we can use a request to **either generate a new valid token, or validate a given token to try to find a valid one.** 

Let's say for intance the number of all possible tokens is 100, than generating 9 new valid tokens (for a total of 10) and than using the last request to take a guess has exactly the same probability (`1/10`) of guessing a valid token by simply using the `10` requests to guess a token.

Things change if we make even just another request at guessing another token. Assuming we didn't find a valid token until now, guessing another token in the first scenario (10 valid tokens) will have a probability of `10/99` (around `1/10` again) while the second scenario will have a probability of `1/90` (almost 10 times less probable than the first scenario). 

Let's formalize the problem a bit more, just so I can plot some nice graphs to show the intuition behind this.

## Some symbols and definitions

Let's suppose we have a alphabet `A`, conisting of `|A|` different simbols. Let's suppose a `D`-length token is a sequence of `D` symbols randomly drawn (with uniform distribution) from `A`. The total number of existing tokens `n` (keyspace size) will be `|A| ^ D`

Our system is protected by an `x` number of `D`-length tokens. At initialization `x=1`.

At every time step `t`, an attacker can either take an attempt at guessing the token or generating a new one, increasing `x` by one. At every time step, `t` will be the total number of requests made by the attacker to the system and `t=g+v` where `g` is the number of tokens generated by the attacker and `v` is the number of attempts at guessing the token.

## Probability of guessing a valid token

At time `t=0`, the probability `P` for an attacker to guess a valid token is `1/n`. At each timestep, if you didn't already guess the token, the keyspace decreases by one meaning that at each timestep, `P` will be `1/(n-t)`.

Let's graph the probability of guessing the token at each timestep with a keyspace of size `n=100`. Let's also graph the cumulative distribution, representing the total probability of guessing the token within `t` steps.

![graph_one](./assets/img/keyspace-deepdive/graph_one.png)
_On the left, the probability of guessing a token given that in the previous step you did not guess it. On the right, the "a priori" probability of guessing the token within `t` steps._

As you can see, with only a valid token the probability of guessing it within `t` steps increases linearly. This makes sense as we expect that by brute-forcing half of the keyspace we'll have a 50/50 chance of guessing a correct token. This is reflected by the cumulative graph, as `P(success) <= n/2` equals `0.5`.

Now let's generate the same graphs, but suppose we have two valid tokens instead of only one from the beginning. At each timestep, given `x` tokens, the probability of guessing a valid one is `x/(n-t)`

![graph_two](./assets/img/keyspace-deepdive/graph_two.png)
_Same graphs as before, but we have two valid tokens instead of one. Notice how a 50% chance of guessing the right token is reached at an earlier timestep than before_

Look at that! The cumulative distribution is not linear anymore, instead the probability increases a lot and a 50% change of guessing the token is already reached at `t=30`.

## The best strategy for an attacker